{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mq180RYbtZIB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import random\n",
        "from google.colab import drive\n",
        "import datetime\n",
        "import sys\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "# imports the torch_xla package\n",
        "# !pip install torch_xla\n",
        "# import torch_xla\n",
        "# import torch_xla.core.xla_model as xm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKPDbeI2y2e0"
      },
      "source": [
        "# Load in Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DXyhfCzx6a5",
        "outputId": "4fcc0189-bd7e-4a08-9e84-6eff6e62674c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bdxpkI2yEUK",
        "outputId": "b429125e-937b-446f-d150-ae21f1ee960b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/dexcom\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/Colab\\ Notebooks/dexcom"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IzDkq6yr-1z1"
      },
      "outputs": [],
      "source": [
        "CHECKPOINT_FOLDER = \"./saved_transformer\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7kDtVlRy5o0"
      },
      "source": [
        "# Util Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysSAcKlitSFR"
      },
      "outputs": [],
      "source": [
        "random.seed(42)\n",
        "\n",
        "# function: parses date string into DateTime Object\n",
        "# input: date\n",
        "# output: DateTime Object\n",
        "def dateParser(date):\n",
        "    mainFormat = '%Y-%m-%d %H:%M:%S.%f'\n",
        "    altFormat = '%Y-%m-%d %H:%M:%S'\n",
        "    try:\n",
        "        return datetime.datetime.strptime(date, mainFormat)\n",
        "    except ValueError:\n",
        "        return datetime.datetime.strptime(date, altFormat)\n",
        "\n",
        "# function: return a DataFrame from directory\n",
        "# input: file directory dexcom\n",
        "# output: DataFrame\n",
        "def getGlucoseData(fileDir):\n",
        "    df = pd.read_csv(fileDir)\n",
        "    data = pd.DataFrame()\n",
        "    data['Time'] = df['Timestamp (YYYY-MM-DDThh:mm:ss)']\n",
        "    data['Glucose'] = pd.to_numeric(df['Glucose Value (mg/dL)'])\n",
        "    data.drop(data.index[:12], inplace=True)\n",
        "    data['Time'] = np.array([dateParser(dateStr) for dateStr in data['Time']])\n",
        "    data['Day'] = np.array([date.day for date in data['Time']])\n",
        "    data = data.reset_index()\n",
        "    return data\n",
        "\n",
        "# function: create samples given glucose data\n",
        "# input: data, length, numSamples\n",
        "# output: np array of samples and value after\n",
        "def sampleTransformer(data, length, numSamples=100):\n",
        "    SOS_token = np.array([301])\n",
        "    EOS_token = np.array([302])\n",
        "    ans = []\n",
        "    for i in range(numSamples):\n",
        "        start = random.randint(0,len(data)- 2 * length - 1)\n",
        "        while True in np.isnan(data[start: start + 2 * length + 1]):\n",
        "            start = random.randint(0,len(data)-2 * length-1)\n",
        "        begin = np.concatenate((SOS_token, data[start : start + length], EOS_token))\n",
        "        end = np.concatenate((SOS_token, data[start + length : start + 2 * length], EOS_token))\n",
        "        ans.append([begin, end])\n",
        "    np.random.shuffle(ans)\n",
        "    return np.array(ans)\n",
        "\n",
        "# function: create a matrix of samples\n",
        "# input: glucoseDict, length, numSamples\n",
        "# output: train_data, val_data\n",
        "def createSamplesArray(glucoseDict, length, numSamples):\n",
        "    shuffled = list(glucoseDict.keys())\n",
        "    np.random.shuffle(shuffled)\n",
        "    train_choice, val_choice = (shuffled[:12], shuffled[12:])\n",
        "    train_data = []\n",
        "    val_data = []\n",
        "    for i in train_choice:\n",
        "        data = glucoseDict[i]\n",
        "        train = sampleTransformer(data, length, numSamples)\n",
        "        for trainVal in train:\n",
        "            train_data.append(trainVal)\n",
        "    for i in val_choice:\n",
        "        data = glucoseDict[i]\n",
        "        val = sampleTransformer(data, length, numSamples)\n",
        "        for validVal in val:\n",
        "            val_data.append(validVal)\n",
        "    train_data = np.array(train_data)\n",
        "    val_data = np.array(val_data)\n",
        "    return train_data, val_data\n",
        "\n",
        "# function: create samples of glucose data regardless of which sample it is from\n",
        "# input: glucoseDict, length, numSamples\n",
        "# output: X, y\n",
        "def createGlucoseSamples(glucoseDict, length, numSamples):\n",
        "    data = sampleTransformer(glucoseDict[random.choice(list(glucoseDict.keys()))], length, numSamples)\n",
        "    random.shuffle(data)\n",
        "    return np.array([data[i][0] for i in range(len(data))]), np.array([data[i][1] for i in range(len(data))])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5e5J9m-y7ff"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IBKFQQn11oTm"
      },
      "outputs": [],
      "source": [
        "class HyperParams:\n",
        "    def __init__(self):\n",
        "        # Constance hyperparameters. They have been tested and don't need to be tuned.\n",
        "        self.NUM_TOKENS = 303\n",
        "        self.NUM_HEADS = 16\n",
        "        self.EMBEDDING_DIM = 16\n",
        "        self.NUM_ENCODER_LAYERS = 4\n",
        "        self.NUM_DECODER_LAYERS = 4\n",
        "        self.DROPOUT_P = 0.5\n",
        "        self.LEARNING_RATE = 0.001\n",
        "        self.WEIGHT_DECAY = 0\n",
        "        self.NORM_FIRST = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CyMoQTTJtlMf"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    # Constructor\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_tokens,\n",
        "        embedding_dim,\n",
        "        num_heads,\n",
        "        num_encoder_layers,\n",
        "        num_decoder_layers,\n",
        "        dropout_p,\n",
        "        norm_first,\n",
        "        device):\n",
        "        super().__init__()\n",
        "\n",
        "        # INFO\n",
        "        self.model_type = \"Transformer\"\n",
        "        self.num_tokens = num_tokens\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.num_encoder_layers = num_encoder_layers\n",
        "        self.num_decoder_layers = num_decoder_layers\n",
        "        self.dropout_p = dropout_p\n",
        "        self.norm_first = norm_first\n",
        "        self.device = device\n",
        "\n",
        "\n",
        "        # LAYERS\n",
        "        self.positional_encoder = PositionalEncoding(\n",
        "            embedding_dim=embedding_dim, dropout_p=dropout_p, max_len=5000, device=self.device\n",
        "        )\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_tokens,\n",
        "            embedding_dim\n",
        "        )\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=self.embedding_dim,\n",
        "            nhead=self.num_heads,\n",
        "            num_encoder_layers=self.num_encoder_layers,\n",
        "            num_decoder_layers=self.num_decoder_layers,\n",
        "            dropout=self.dropout_p,\n",
        "            norm_first=self.norm_first\n",
        "        )\n",
        "        self.layer_norm = nn.LayerNorm(embedding_dim)\n",
        "        self.out = nn.Linear(embedding_dim, num_tokens)\n",
        "\n",
        "    # function: forward of model\n",
        "    # input: src, tgt, tgt_mask\n",
        "    # output: output after forward run through model\n",
        "    def forward(self, src, tgt, tgt_mask=None):\n",
        "        # Src size must be (batch_size, src, sequence_length)\n",
        "        # Tgt size must be (batch_size, tgt, sequence_length)\n",
        "\n",
        "        src = self.embedding(src) * math.sqrt(self.embedding_dim)\n",
        "        tgt = self.embedding(tgt) * math.sqrt(self.embedding_dim)\n",
        "        src = self.positional_encoder(src)\n",
        "        tgt = self.positional_encoder(tgt)\n",
        "\n",
        "        # permute to have batch_size come first\n",
        "        src = src.permute(1,0,2)\n",
        "        tgt = tgt.permute(1,0,2)\n",
        "\n",
        "        # layer normalization for quicker training of transformer model\n",
        "        # transformer model\n",
        "        transformer_out = self.transformer(src, tgt, tgt_mask=tgt_mask)\n",
        "\n",
        "        out = self.out(transformer_out)\n",
        "\n",
        "        return out\n",
        "\n",
        "    # function: creates a mask with 0's in bottom left of matrix\n",
        "    # input: size\n",
        "    # output: mask\n",
        "    def get_tgt_mask(self, size) -> torch.tensor:\n",
        "        mask = torch.tril(torch.ones(size,size) * float('-inf')).T\n",
        "        for i in range(size):\n",
        "            mask[i, i] = 0\n",
        "        return mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWgrn8Qit7dy"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, embedding_dim, dropout_p, max_len, device):\n",
        "        super().__init__()\n",
        "\n",
        "        # Layers\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "        self.positional_encoding = self.get_positional_encoding(max_len, embedding_dim).to(device)\n",
        "\n",
        "    def get_positional_encoding(self, max_len, embedding_dim):\n",
        "        positional_encoding = torch.zeros(max_len, embedding_dim)\n",
        "        # column of positions\n",
        "        position = torch.arange(0, max_len, dtype=torch.float32).view(-1,1)\n",
        "        # division term\n",
        "        div_term = torch.exp(torch.arange(0, embedding_dim, 2, dtype=torch.float32) * (-math.log(10000.0) / embedding_dim))\n",
        "        # even numbered positional encoding\n",
        "        positional_encoding[:, 0::2] = torch.sin(position * div_term)\n",
        "        # odd numbered positional encoding\n",
        "        positional_encoding[:, 1::2] = torch.cos(position * div_term)\n",
        "        positional_encoding = positional_encoding.unsqueeze(0).transpose(0,1)\n",
        "        return positional_encoding\n",
        "\n",
        "    def forward(self, token_embedding: torch.tensor) -> torch.tensor:\n",
        "        # Residual connection + pos encoding\n",
        "        return self.dropout(token_embedding + self.positional_encoding[:token_embedding.size(0), :])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyzPB_Lg1TIv"
      },
      "source": [
        "# Get glucose data from Dexcom files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fB7d-WsQtwAn"
      },
      "outputs": [],
      "source": [
        "dexcomFiles = [f\"Dexcom_{str(i).zfill(3)}.csv\" for i in range(1, 17)]\n",
        "glucoseDict = {}\n",
        "idx = 0\n",
        "for file in dexcomFiles:\n",
        "  idx += 1\n",
        "  df = getGlucoseData(file)\n",
        "  glucoseDict[idx] = df['Glucose'].to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpAtY1_N2dd0"
      },
      "outputs": [],
      "source": [
        "# train_data, val_data = createSamplesArray(glucoseDict, 12, 1000)\n",
        "X,y = createGlucoseSamples(glucoseDict, 12, 10000)\n",
        "# get X_train, X_test, X_val (7:1:2), and y_train, y_test, y_val (7:1:2)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.125, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mAXUZbW86ZH",
        "outputId": "3d1a7333-eab3-4911-af81-6cc142bda7a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "546 batches of size 16\n",
            "124 batches of size 16\n",
            "78 batches of size 16\n"
          ]
        }
      ],
      "source": [
        "def createBatches(data, batch_size=16, padding=False, padding_token=-1):\n",
        "    batches = []\n",
        "    idx = 0\n",
        "    while idx + batch_size < len(data):\n",
        "        batches.append(np.array(data[idx : idx + batch_size]).astype(int))\n",
        "        idx += batch_size\n",
        "    print(f\"{len(batches)} batches of size {batch_size}\")\n",
        "    return batches\n",
        "\n",
        "train_data = [[X_train[i], y_train[i]] for i in range(len(X_train))]\n",
        "val_data = [[X_val[i], y_val[i]] for i in range(len(X_val))]\n",
        "test_data = [[X_test[i], y_test[i]] for i in range(len(X_test))]\n",
        "train_dataloader = createBatches(train_data)\n",
        "val_dataloader = createBatches(val_data)\n",
        "test_dataloader = createBatches(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnpPM6Wo_qNE"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSJUwA6_t1xk",
        "outputId": "b9b45a28-63a4-4853-c5f5-b985120862d7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "hyperparams = HyperParams()\n",
        "model = Transformer(\n",
        "    num_tokens=hyperparams.NUM_TOKENS,\n",
        "    embedding_dim=hyperparams.EMBEDDING_DIM,\n",
        "    num_heads=hyperparams.NUM_HEADS,\n",
        "    num_encoder_layers=hyperparams.NUM_ENCODER_LAYERS,\n",
        "    num_decoder_layers=hyperparams.NUM_DECODER_LAYERS,\n",
        "    dropout_p=hyperparams.DROPOUT_P,\n",
        "    norm_first=hyperparams.NORM_FIRST,\n",
        "    device = device\n",
        ").to(device)\n",
        "opt = optim.Adam(model.parameters(), lr=hyperparams.LEARNING_RATE, weight_decay=hyperparams.WEIGHT_DECAY, eps=1e-6)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "loss_fn = loss_fn.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "asRUl0xCtpva"
      },
      "outputs": [],
      "source": [
        "def train(model, opt, loss_fn, dataloader):\n",
        "\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    accuracy = []\n",
        "\n",
        "    for batch in dataloader:\n",
        "        # X --> first half\n",
        "        # y --> second half\n",
        "        X, y = batch[:, 0], batch[:, 1]\n",
        "        X, y = torch.tensor(X).to(device), torch.tensor(y).to(device)\n",
        "        # Now we shift the tgt by one so with the <SOS> we predict the token at pos 1\n",
        "        # y_input [SOS_token, ...]\n",
        "        y_input = y[:,:-1]\n",
        "        # y_expected [..., EOS]\n",
        "        y_expected = y[:,1:]\n",
        "\n",
        "        # Get mask to mask out the next words\n",
        "        sequence_length = len(y_input[0])\n",
        "        tgt_mask = model.get_tgt_mask(sequence_length).to(device)\n",
        "\n",
        "        # Standard training except we pass in y_input and tgt_mask\n",
        "        pred = model(X, y_input, tgt_mask)\n",
        "\n",
        "        # Permute pred to have batch size first again\n",
        "        pred = pred.permute(1, 2, 0)\n",
        "        y_pred = torch.argmax(pred.detach(), axis=1)\n",
        "        mape = mean_absolute_percentage_error(y_expected, y_pred)\n",
        "        accuracy.append(100 - mape.cpu().detach().item())\n",
        "        loss = loss_fn(pred, y_expected)\n",
        "\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        total_loss += loss.detach().item()\n",
        "    epoch_loss = total_loss / len(dataloader)\n",
        "    epoch_acc = np.mean(accuracy)\n",
        "\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "\n",
        "def evaluate(model, loss_fn, dataloader):\n",
        "\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "\n",
        "    accuracy = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            X, y = batch[:, 0], batch[:, 1]\n",
        "            X, y = torch.tensor(X, dtype=torch.long, device=device), torch.tensor(y, dtype=torch.long, device=device)\n",
        "\n",
        "            # Now we shift the tgt by one so with the <SOS> we predict the token at pos 1\n",
        "            # y_input [SOS_token, ...]\n",
        "            y_input = y[:,:-1]\n",
        "            # y_expected [..., EOS]\n",
        "            y_expected = y[:,1:]\n",
        "\n",
        "            # Get mask to mask out the next words\n",
        "            sequence_length = len(y_input[0])\n",
        "            tgt_mask = model.get_tgt_mask(sequence_length).to(device)\n",
        "            pred = model(X, y_input, tgt_mask)\n",
        "\n",
        "            # Permute pred to have batch size first again\n",
        "            pred = pred.permute(1, 2, 0)\n",
        "            y_pred = torch.argmax(pred, axis=1)\n",
        "            mape = mean_absolute_percentage_error(y_expected, y_pred)\n",
        "            accuracy.append(100 - mape.cpu().detach().item())\n",
        "            loss = loss_fn(pred, y_expected)\n",
        "            total_loss += loss.detach().item()\n",
        "\n",
        "        # sanity check\n",
        "        # print(y_pred[0][1:])\n",
        "        # print(y_expected[0][:-1])\n",
        "\n",
        "    epoch_loss = total_loss / len(dataloader)\n",
        "    epoch_acc = np.mean(accuracy)\n",
        "\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "def mean_absolute_percentage_error(y_true, y_pred):\n",
        "    return torch.mean(torch.abs((y_true[:, :-1] - y_pred[:, 1:])) / y_true[:, :-1]) * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CT8NBgLGttjq",
        "outputId": "c1729c9f-2c31-4692-a34a-7d06236975af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training and validating model\n",
            "------------------------- Epoch 1 -------------------------\n",
            "Training loss: 4.1770\n",
            "Training accuracy: -30.8233\n",
            "Validation loss: 3.3716\n",
            "Validation accuracy: 73.8758\n",
            "Saving ...\n",
            "Test loss: 3.3609\n",
            "Test accuracy: 73.6626\n",
            "------------------------- Epoch 2 -------------------------\n",
            "Training loss: 3.3731\n",
            "Training accuracy: 56.6853\n",
            "Validation loss: 2.8706\n",
            "Validation accuracy: 93.9534\n",
            "Saving ...\n",
            "Test loss: 2.8591\n",
            "Test accuracy: 93.9754\n",
            "------------------------- Epoch 3 -------------------------\n",
            "Training loss: 3.0866\n",
            "Training accuracy: 69.7497\n",
            "Validation loss: 2.7049\n",
            "Validation accuracy: 97.1657\n",
            "Saving ...\n",
            "Test loss: 2.6940\n",
            "Test accuracy: 97.1518\n",
            "------------------------- Epoch 4 -------------------------\n",
            "Training loss: 2.9471\n",
            "Training accuracy: 72.4043\n",
            "Validation loss: 2.6169\n",
            "Validation accuracy: 98.2234\n",
            "Saving ...\n",
            "Test loss: 2.6054\n",
            "Test accuracy: 98.2474\n",
            "------------------------- Epoch 5 -------------------------\n",
            "Training loss: 2.8470\n",
            "Training accuracy: 74.0237\n",
            "Validation loss: 2.5485\n",
            "Validation accuracy: 98.5405\n",
            "Saving ...\n",
            "Test loss: 2.5359\n",
            "Test accuracy: 98.5961\n",
            "------------------------- Epoch 6 -------------------------\n",
            "Training loss: 2.7825\n",
            "Training accuracy: 74.5562\n",
            "Validation loss: 2.5086\n",
            "Validation accuracy: 98.6428\n",
            "Saving ...\n",
            "Test loss: 2.4953\n",
            "Test accuracy: 98.6332\n",
            "------------------------- Epoch 7 -------------------------\n",
            "Training loss: 2.7286\n",
            "Training accuracy: 74.8562\n",
            "Validation loss: 2.4765\n",
            "Validation accuracy: 98.8626\n",
            "Saving ...\n",
            "Test loss: 2.4631\n",
            "Test accuracy: 98.8441\n",
            "------------------------- Epoch 8 -------------------------\n",
            "Training loss: 2.6869\n",
            "Training accuracy: 75.2109\n",
            "Validation loss: 2.4417\n",
            "Validation accuracy: 98.9109\n",
            "Saving ...\n",
            "Test loss: 2.4282\n",
            "Test accuracy: 98.8968\n",
            "------------------------- Epoch 9 -------------------------\n",
            "Training loss: 2.6534\n",
            "Training accuracy: 75.5815\n",
            "Validation loss: 2.4109\n",
            "Validation accuracy: 98.6994\n",
            "Saving ...\n",
            "Test loss: 2.3984\n",
            "Test accuracy: 98.7056\n",
            "------------------------- Epoch 10 -------------------------\n",
            "Training loss: 2.6297\n",
            "Training accuracy: 75.8549\n",
            "Validation loss: 2.3866\n",
            "Validation accuracy: 98.3581\n",
            "Saving ...\n",
            "Test loss: 2.3731\n",
            "Test accuracy: 98.2757\n",
            "------------------------- Epoch 11 -------------------------\n",
            "Training loss: 2.6089\n",
            "Training accuracy: 75.4684\n",
            "Validation loss: 2.3681\n",
            "Validation accuracy: 98.7677\n",
            "Saving ...\n",
            "Test loss: 2.3537\n",
            "Test accuracy: 98.7454\n",
            "------------------------- Epoch 12 -------------------------\n",
            "Training loss: 2.5848\n",
            "Training accuracy: 75.4498\n",
            "Validation loss: 2.3448\n",
            "Validation accuracy: 98.0503\n",
            "Saving ...\n",
            "Test loss: 2.3313\n",
            "Test accuracy: 98.0188\n",
            "------------------------- Epoch 13 -------------------------\n",
            "Training loss: 2.5648\n",
            "Training accuracy: 75.0650\n",
            "Validation loss: 2.3300\n",
            "Validation accuracy: 98.1099\n",
            "Saving ...\n",
            "Test loss: 2.3163\n",
            "Test accuracy: 98.0947\n",
            "------------------------- Epoch 14 -------------------------\n",
            "Training loss: 2.5501\n",
            "Training accuracy: 75.6752\n",
            "Validation loss: 2.3129\n",
            "Validation accuracy: 97.9995\n",
            "Saving ...\n",
            "Test loss: 2.2993\n",
            "Test accuracy: 97.9296\n",
            "------------------------- Epoch 15 -------------------------\n",
            "Training loss: 2.5319\n",
            "Training accuracy: 75.7434\n",
            "Validation loss: 2.3006\n",
            "Validation accuracy: 97.6753\n",
            "Saving ...\n",
            "Test loss: 2.2869\n",
            "Test accuracy: 97.6183\n",
            "------------------------- Epoch 16 -------------------------\n",
            "Training loss: 2.5196\n",
            "Training accuracy: 75.4066\n",
            "Validation loss: 2.2800\n",
            "Validation accuracy: 97.4847\n",
            "Saving ...\n",
            "Test loss: 2.2670\n",
            "Test accuracy: 97.4598\n",
            "------------------------- Epoch 17 -------------------------\n",
            "Training loss: 2.5109\n",
            "Training accuracy: 75.2529\n",
            "Validation loss: 2.2792\n",
            "Validation accuracy: 97.9438\n",
            "Saving ...\n",
            "Test loss: 2.2659\n",
            "Test accuracy: 97.9016\n",
            "------------------------- Epoch 18 -------------------------\n",
            "Training loss: 2.4987\n",
            "Training accuracy: 75.4722\n",
            "Validation loss: 2.2618\n",
            "Validation accuracy: 97.4826\n",
            "Saving ...\n",
            "Test loss: 2.2480\n",
            "Test accuracy: 97.5547\n",
            "------------------------- Epoch 19 -------------------------\n",
            "Training loss: 2.4870\n",
            "Training accuracy: 75.7594\n",
            "Validation loss: 2.2517\n",
            "Validation accuracy: 96.4087\n",
            "Saving ...\n",
            "Test loss: 2.2381\n",
            "Test accuracy: 96.4860\n",
            "------------------------- Epoch 20 -------------------------\n",
            "Training loss: 2.4797\n",
            "Training accuracy: 75.4491\n",
            "Validation loss: 2.2340\n",
            "Validation accuracy: 96.9073\n",
            "Saving ...\n",
            "Test loss: 2.2207\n",
            "Test accuracy: 97.0301\n",
            "Test accuracy after 20 epochs: 98.89675798935768\n"
          ]
        }
      ],
      "source": [
        "def fit(model, opt, loss_fn, train_dataloader, val_dataloader, test_dataloader, epochs):\n",
        "\n",
        "    # Used for plotting later on\n",
        "    train_loss_list, validation_loss_list = [], []\n",
        "\n",
        "    best_test_acc = float('-inf')\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    print(\"Training and validating model\")\n",
        "    for epoch in range(epochs):\n",
        "        print(\"-\"*25, f\"Epoch {epoch + 1}\",\"-\"*25)\n",
        "\n",
        "        train_loss, train_accuracy = train(model, opt, loss_fn, train_dataloader)\n",
        "        train_loss_list += [train_loss]\n",
        "\n",
        "        validation_loss, validation_accuracy = evaluate(model, loss_fn, val_dataloader)\n",
        "        validation_loss_list += [validation_loss]\n",
        "\n",
        "\n",
        "\n",
        "        print(f\"Training loss: {train_loss:.4f}\")\n",
        "        print(f\"Training accuracy: {train_accuracy:.4f}\")\n",
        "        print(f\"Validation loss: {validation_loss:.4f}\")\n",
        "        print(f\"Validation accuracy: {validation_accuracy:.4f}\")\n",
        "        if validation_loss < best_val_loss:\n",
        "            best_val_loss = validation_loss\n",
        "            if not os.path.exists(CHECKPOINT_FOLDER):\n",
        "                os.makedirs(CHECKPOINT_FOLDER)\n",
        "            print(\"Saving ...\")\n",
        "            state = {'state_dict': model.state_dict(),\n",
        "                    'epoch': epoch,\n",
        "                    'lr': 0.001}\n",
        "            torch.save(state['state_dict'], os.path.join(CHECKPOINT_FOLDER, 'ermTransformer.pth'))\n",
        "\n",
        "        test_loss, test_accuracy = evaluate(model, loss_fn, test_dataloader)\n",
        "        print(f\"Test loss: {test_loss:.4f}\")\n",
        "        print(f\"Test accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "        if test_accuracy > best_test_acc:\n",
        "            best_test_acc = test_accuracy\n",
        "\n",
        "    print(f\"Test accuracy after {epochs} epochs: {best_test_acc}\")\n",
        "    return train_loss_list, validation_loss_list\n",
        "\n",
        "train_loss_list, validation_loss_list = fit(model, opt, loss_fn, train_dataloader, val_dataloader, test_dataloader, 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5K83HJ1Mz3qw"
      },
      "source": [
        "# Leave One Person Out Cross Validation (LOPOCV)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZaOjFEfNKOMn"
      },
      "outputs": [],
      "source": [
        "def fit(model, opt, loss_fn, train_dataloader, val_dataloader, test_dataloader, epochs):\n",
        "\n",
        "    # Used for plotting later on\n",
        "    train_loss_list, validation_loss_list = [], []\n",
        "\n",
        "    best_test_acc = float('-inf')\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    print(\"Training and validating model\")\n",
        "    for epoch in tqdm(range(epochs), total=epochs):\n",
        "        # print(\"-\"*25, f\"Epoch {epoch + 1}\",\"-\"*25)\n",
        "\n",
        "        train_loss, train_accuracy = train(model, opt, loss_fn, train_dataloader)\n",
        "        train_loss_list += [train_loss]\n",
        "\n",
        "        validation_loss, validation_accuracy = evaluate(model, loss_fn, val_dataloader)\n",
        "        validation_loss_list += [validation_loss]\n",
        "\n",
        "        # print(f\"Training loss: {train_loss:.4f}\")\n",
        "        # print(f\"Training accuracy: {train_accuracy:.4f}\")\n",
        "        # print(f\"Validation loss: {validation_loss:.4f}\")\n",
        "        # print(f\"Validation accuracy: {validation_accuracy:.4f}\")\n",
        "        \"\"\"if validation_loss < best_val_loss:\n",
        "            best_val_loss = validation_loss\n",
        "            if not os.path.exists(CHECKPOINT_FOLDER):\n",
        "                os.makedirs(CHECKPOINT_FOLDER)\n",
        "            print(\"Saving ...\")\n",
        "            state = {'state_dict': model.state_dict(),\n",
        "                    'epoch': epoch,\n",
        "                    'lr': 0.001}\n",
        "            torch.save(state['state_dict'], os.path.join(CHECKPOINT_FOLDER, 'ermTransformer.pth'))\"\"\"\n",
        "\n",
        "        test_loss, test_accuracy = evaluate(model, loss_fn, test_dataloader)\n",
        "        # print(f\"Test loss: {test_loss:.4f}\")\n",
        "        # print(f\"Test accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "        if test_accuracy > best_test_acc:\n",
        "            best_test_acc = test_accuracy\n",
        "\n",
        "    print(f\"Test accuracy after {epochs} epochs: {best_test_acc}\")\n",
        "    return train_loss_list, validation_loss_list, best_test_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAuEBCoNIses",
        "outputId": "4d9db468-4613-4b3f-e8cf-501bcc95a7ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Leaving out subject 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "437 batches of size 16\n",
            "124 batches of size 16\n",
            "62 batches of size 16\n",
            "Training and validating model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [10:15<00:00, 61.52s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy after 10 epochs: 96.5121211736433\n",
            "Leaving out subject 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "437 batches of size 16\n",
            "124 batches of size 16\n",
            "62 batches of size 16\n",
            "Training and validating model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [10:14<00:00, 61.44s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy after 10 epochs: 90.44678805720422\n",
            "Leaving out subject 3\n",
            "437 batches of size 16\n",
            "124 batches of size 16\n",
            "62 batches of size 16\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training and validating model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [10:18<00:00, 61.84s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy after 10 epochs: 88.76579225063324\n",
            "Leaving out subject 4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "437 batches of size 16\n",
            "124 batches of size 16\n",
            "62 batches of size 16\n",
            "Training and validating model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [10:16<00:00, 61.68s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy after 10 epochs: 95.0692483571268\n",
            "Leaving out subject 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "437 batches of size 16\n",
            "124 batches of size 16\n",
            "62 batches of size 16\n",
            "Training and validating model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [10:17<00:00, 61.72s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy after 10 epochs: 88.12829586959654\n",
            "Leaving out subject 6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "437 batches of size 16\n",
            "124 batches of size 16\n",
            "62 batches of size 16\n",
            "Training and validating model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [10:52<00:00, 65.30s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy after 10 epochs: 88.97202157974243\n",
            "Leaving out subject 7\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "437 batches of size 16\n",
            "124 batches of size 16\n",
            "62 batches of size 16\n",
            "Training and validating model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [10:48<00:00, 64.81s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy after 10 epochs: 72.713529757915\n",
            "Leaving out subject 8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "437 batches of size 16\n",
            "124 batches of size 16\n",
            "62 batches of size 16\n",
            "Training and validating model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [10:53<00:00, 65.30s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy after 10 epochs: 92.67791716898641\n",
            "Leaving out subject 9\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "437 batches of size 16\n",
            "124 batches of size 16\n",
            "62 batches of size 16\n",
            "Training and validating model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [10:22<00:00, 62.20s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy after 10 epochs: 93.87542084532399\n",
            "Leaving out subject 10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "437 batches of size 16\n",
            "124 batches of size 16\n",
            "62 batches of size 16\n",
            "Training and validating model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [10:18<00:00, 61.89s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy after 10 epochs: 86.92322143047086\n",
            "Leaving out subject 11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "437 batches of size 16\n",
            "124 batches of size 16\n",
            "62 batches of size 16\n",
            "Training and validating model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [10:14<00:00, 61.49s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy after 10 epochs: 92.9126719563238\n",
            "Leaving out subject 12\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "437 batches of size 16\n",
            "124 batches of size 16\n",
            "62 batches of size 16\n",
            "Training and validating model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [10:06<00:00, 60.67s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy after 10 epochs: 93.96207946731198\n",
            "Leaving out subject 13\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "437 batches of size 16\n",
            "124 batches of size 16\n",
            "62 batches of size 16\n",
            "Training and validating model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [10:05<00:00, 60.55s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy after 10 epochs: 93.91763395263303\n",
            "Leaving out subject 14\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "437 batches of size 16\n",
            "124 batches of size 16\n",
            "62 batches of size 16\n",
            "Training and validating model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [10:06<00:00, 60.63s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy after 10 epochs: 95.28384989307773\n",
            "Leaving out subject 15\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "437 batches of size 16\n",
            "124 batches of size 16\n",
            "62 batches of size 16\n",
            "Training and validating model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [10:10<00:00, 61.03s/it]\n",
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n",
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n",
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy after 10 epochs: 97.0989654160315\n",
            "Leaving out subject 16\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-42-1bf920425f09>\", line 9, in <cell line: 2>\n",
            "    df = getGlucoseData(file)\n",
            "  File \"<ipython-input-5-cbcdc4bd728a>\", line 18, in getGlucoseData\n",
            "    df = pd.read_csv(fileDir)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\", line 211, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\", line 331, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\", line 950, in read_csv\n",
            "    return _read(filepath_or_buffer, kwds)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\", line 605, in _read\n",
            "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\", line 1442, in __init__\n",
            "    self._engine = self._make_engine(f, self.engine)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\", line 1735, in _make_engine\n",
            "    self.handles = get_handle(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\", line 856, in get_handle\n",
            "    handle = open(\n",
            "OSError: [Errno 107] Transport endpoint is not connected: 'Dexcom_001.csv'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'OSError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1620, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n",
            "    module = getmodule(object, filename)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 861, in getmodule\n",
            "    file = getabsfile(object, _filename)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 845, in getabsfile\n",
            "    return os.path.normcase(os.path.abspath(_filename))\n",
            "  File \"/usr/lib/python3.10/posixpath.py\", line 384, in abspath\n",
            "    cwd = os.getcwd()\n",
            "OSError: [Errno 107] Transport endpoint is not connected\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-42-1bf920425f09>\", line 9, in <cell line: 2>\n",
            "    df = getGlucoseData(file)\n",
            "  File \"<ipython-input-5-cbcdc4bd728a>\", line 18, in getGlucoseData\n",
            "    df = pd.read_csv(fileDir)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\", line 211, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\", line 331, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\", line 950, in read_csv\n",
            "    return _read(filepath_or_buffer, kwds)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\", line 605, in _read\n",
            "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\", line 1442, in __init__\n",
            "    self._engine = self._make_engine(f, self.engine)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\", line 1735, in _make_engine\n",
            "    self.handles = get_handle(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\", line 856, in get_handle\n",
            "    handle = open(\n",
            "OSError: [Errno 107] Transport endpoint is not connected: 'Dexcom_001.csv'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'OSError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n",
            "    self.showtraceback(running_compiled_code=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n",
            "    stb = self.InteractiveTB.structured_traceback(etype,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n",
            "    return FormattedTB.structured_traceback(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n",
            "    return VerboseTB.structured_traceback(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1124, in structured_traceback\n",
            "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n",
            "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n",
            "    return len(records), 0\n",
            "TypeError: object of type 'NoneType' has no len()\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1620, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n",
            "    module = getmodule(object, filename)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 861, in getmodule\n",
            "    file = getabsfile(object, _filename)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 845, in getabsfile\n",
            "    return os.path.normcase(os.path.abspath(_filename))\n",
            "  File \"/usr/lib/python3.10/posixpath.py\", line 384, in abspath\n",
            "    cwd = os.getcwd()\n",
            "OSError: [Errno 107] Transport endpoint is not connected\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-42-1bf920425f09>\", line 9, in <cell line: 2>\n",
            "    df = getGlucoseData(file)\n",
            "  File \"<ipython-input-5-cbcdc4bd728a>\", line 18, in getGlucoseData\n",
            "    df = pd.read_csv(fileDir)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\", line 211, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\", line 331, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\", line 950, in read_csv\n",
            "    return _read(filepath_or_buffer, kwds)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\", line 605, in _read\n",
            "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\", line 1442, in __init__\n",
            "    self._engine = self._make_engine(f, self.engine)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\", line 1735, in _make_engine\n",
            "    self.handles = get_handle(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\", line 856, in get_handle\n",
            "    handle = open(\n",
            "OSError: [Errno 107] Transport endpoint is not connected: 'Dexcom_001.csv'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'OSError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n",
            "    self.showtraceback(running_compiled_code=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n",
            "    stb = self.InteractiveTB.structured_traceback(etype,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n",
            "    return FormattedTB.structured_traceback(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n",
            "    return VerboseTB.structured_traceback(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1124, in structured_traceback\n",
            "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n",
            "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n",
            "    return len(records), 0\n",
            "TypeError: object of type 'NoneType' has no len()\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3492, in run_ast_nodes\n",
            "    self.showtraceback()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n",
            "    stb = self.InteractiveTB.structured_traceback(etype,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n",
            "    return FormattedTB.structured_traceback(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n",
            "    return VerboseTB.structured_traceback(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1142, in structured_traceback\n",
            "    formatted_exceptions += self.format_exception_as_a_whole(etype, evalue, etb, lines_of_context,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n",
            "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n",
            "    return len(records), 0\n",
            "TypeError: object of type 'NoneType' has no len()\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1620, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n",
            "    module = getmodule(object, filename)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 861, in getmodule\n",
            "    file = getabsfile(object, _filename)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 845, in getabsfile\n",
            "    return os.path.normcase(os.path.abspath(_filename))\n",
            "  File \"/usr/lib/python3.10/posixpath.py\", line 384, in abspath\n",
            "    cwd = os.getcwd()\n",
            "OSError: [Errno 107] Transport endpoint is not connected\n"
          ]
        }
      ],
      "source": [
        "best_test_accs = []\n",
        "for lopo in range(16):\n",
        "  print(f\"Leaving out subject {lopo+1}\")\n",
        "  dexcomFiles = [f\"Dexcom_{str(i).zfill(3)}.csv\" for i in range(1, 17) if i != lopo+1]\n",
        "  glucoseDict = {}\n",
        "  idx = 0\n",
        "  for file in dexcomFiles:\n",
        "    idx += 1\n",
        "    df = getGlucoseData(file)\n",
        "    glucoseDict[idx] = df['Glucose'].to_numpy()\n",
        "  # train_data, val_data = createSamplesArray(glucoseDict, 12, 1000)\n",
        "  X,y = createGlucoseSamples(glucoseDict, 12, 9000)\n",
        "  X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=float(2/9), random_state=42)\n",
        "  # get test data\n",
        "  glucoseDictTest = {}\n",
        "  glucoseDictTest[f\"Dexcom_{str(lopo+1).zfill(3)}.csv\"] = getGlucoseData(f\"Dexcom_{str(lopo+1).zfill(3)}.csv\")['Glucose'].to_numpy()\n",
        "  X_test, y_test = createGlucoseSamples(glucoseDictTest, 12, 1000)\n",
        "  # get X_train, X_test, X_val (7:1:2), and y_train, y_test, y_val (7:1:2)\n",
        "  train_data = [[X_train[i], y_train[i]] for i in range(len(X_train))]\n",
        "  val_data = [[X_val[i], y_val[i]] for i in range(len(X_val))]\n",
        "  test_data = [[X_test[i], y_test[i]] for i in range(len(X_test))]\n",
        "  train_dataloader = createBatches(train_data)\n",
        "  val_dataloader = createBatches(val_data)\n",
        "  test_dataloader = createBatches(test_data)\n",
        "\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  hyperparams = HyperParams()\n",
        "  model = Transformer(\n",
        "      num_tokens=hyperparams.NUM_TOKENS,\n",
        "      embedding_dim=hyperparams.EMBEDDING_DIM,\n",
        "      num_heads=hyperparams.NUM_HEADS,\n",
        "      num_encoder_layers=hyperparams.NUM_ENCODER_LAYERS,\n",
        "      num_decoder_layers=hyperparams.NUM_DECODER_LAYERS,\n",
        "      dropout_p=hyperparams.DROPOUT_P,\n",
        "      norm_first=hyperparams.NORM_FIRST,\n",
        "      device = device\n",
        "  ).to(device)\n",
        "  opt = optim.Adam(model.parameters(), lr=hyperparams.LEARNING_RATE, weight_decay=hyperparams.WEIGHT_DECAY, eps=1e-6)\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "  loss_fn = loss_fn.to(device)\n",
        "\n",
        "  train_loss_list, validation_loss_list, best_test_acc = fit(model, opt, loss_fn, train_dataloader, val_dataloader, test_dataloader, 10)\n",
        "  best_test_accs.append(best_test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OcQ3PTYUVoxx"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}